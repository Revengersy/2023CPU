{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03eded80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "\n",
    "#a\n",
    "#\n",
    "# Set options & Paths\n",
    "pd.set_option('display.max_columns', None)\n",
    "directory_path = r'C:\\Users\\FILAB\\Desktop\\230711_자동차 산업'\n",
    "#\n",
    "\n",
    "EV_df = pd.read_excel(directory_path + \"\\\\230720_EV_df.xlsx\")\n",
    "# EV_battery_df['텍스트'] = EV_battery_df['발명의 명칭'] + \" \" + EV_battery_df['요약'] + \" \" + EV_battery_df['대표청구항']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_df = EV_df.iloc[:,1:]\n",
    "EV_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb98c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumetable(df, max_unique=7):\n",
    "    print(f'데이터셋 형상: {df.shape}')\n",
    "    summary = pd.DataFrame(df.dtypes, columns = ['데이터 타입'])\n",
    "    summary = summary.reset_index()\n",
    "    summary = summary.rename(columns = {'index': '피처'})\n",
    "    summary['결측값 개수'] = df.isnull().sum().values\n",
    "    summary['고윳값 개수'] = df.nunique().values\n",
    "    summary['첫 번째 값'] = df.loc[0].values\n",
    "    summary['두 번째 값'] = df.loc[1].values\n",
    "    summary['세 번째 값'] = df.loc[2].values\n",
    "    summary['고윳값예시'] = df.apply(lambda x: list(x.unique()) if x.nunique() <= max_unique else ', '.join(str(val) for val in list(x.unique()[:4]) + ['...']), axis=0).values\n",
    "\n",
    "\n",
    "    return summary\n",
    "\n",
    "def print_unique(summary):\n",
    "    for index, row in summary.iterrows():\n",
    "        print(f\"{row['피처']} : {row['고윳값 예시']}\")\n",
    "\n",
    "resume_df = resumetable(EV_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db080c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_df['Current CPC Main 4 digit'] = EV_df['Current CPC Main'].str.extract(r'(....)')\n",
    "EV_df['Current CPC Main 8 digit'] = EV_df['Current CPC Main'].str.extract(r'(.+/)')\n",
    "EV_df['Current CPC Main 8 digit'] = EV_df['Current CPC Main 8 digit'].str.replace(\"\\/\", \"\")\n",
    "EV_df['Current CPC Main 8 digit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e833031b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc2b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인용 자료 확보\n",
    "\n",
    "Patent_df = pd.read_csv(directory_path + \"\\\\BWD.txt\")\n",
    "Patent_df\n",
    "\n",
    "temp_df = EV_df.merge(Patent_df, left_on=\"등록번호\", right_on = \"PatNum\", how= 'left')\n",
    "\n",
    "\n",
    "\n",
    "temp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e894ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_df = temp_df.loc[:, ['BWD', '등록번호']].copy()\n",
    "citation_df.columns = ['source', 'target']\n",
    "\n",
    "citation_df.dropna(inplace=True)\n",
    "citation_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b696e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_patents = set(EV_df['등록번호'])\n",
    "citation_df = citation_df[citation_df['source'].isin(EV_patents)]\n",
    "citation_df\n",
    "# idx_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Patent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa0a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "#최원준 데이터 버전\n",
    "\n",
    "def df_to_kp(df):\n",
    "    \"\"\"\n",
    "    Get Dataframe with Knowledge Persistence scores of each directed path between two nodes(source-target)\n",
    "\n",
    "    A directed path is a type of graph and can be defined with relations of many nodes\n",
    "    Knowledge Persistence(KP) is a score that measures the level of influence in the knowledge flow.\n",
    "    (especially the knowledge flow of patent data)\n",
    "    It can be the basis of Main Path Analysis.\n",
    "\n",
    "    ▷ Parameters\n",
    "    ----------\n",
    "    source_target : list\n",
    "        e.g.\n",
    "        [['A', 'B'],\n",
    "        ['B', 'C'],\n",
    "        ['C', 'D']]\n",
    "\n",
    "    ▷ Returns\n",
    "    -------\n",
    "    pd.Dataframe\n",
    "        columns : 'target', 'kp'\n",
    "\n",
    "    ▷ Notes\n",
    "    -----\n",
    "    ...\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    # Backward weighting all the edges(citations)\n",
    "    # target 노드에 대한 1/number of bwd citation\n",
    "    weight = df.groupby(['target'])['source'].count().reset_index()\n",
    "    step_1 = df.merge(weight, on=\"target\", how=\"left\")\n",
    "    step_1.columns = ['source', 'target', 'weight']\n",
    "    step_1['weight'] = 1 / step_1['weight']\n",
    "\n",
    "    # Endpoint 찾기\n",
    "    step_2 = step_1[['source', 'target']]\n",
    "    step_2 = step_2.groupby(['source'])['target'].count().reset_index()\n",
    "    step_2.columns = ['target', 'countif']\n",
    "    step_3 = step_1.merge(step_2, on='target', how='left')\n",
    "    step_3\n",
    "\n",
    "    # end point 및 바로 전에 있는 kp 계산 및 새로운 DataFrame 생성\n",
    "    step_4 = step_3[step_3['countif'].isnull()]  # end point에 대한 선택\n",
    "    KP = step_4[['target']]  # end point를 찾아서 새로운 DataFrame 생성\n",
    "    KP = KP.drop_duplicates(['target'])  # end point 중 중복된 것 중 하나만 남김\n",
    "    KP['kp'] = 0  # endpoint의 kp지수는 0을 추가\n",
    "    step_4 = step_4.groupby(['source']).sum().reset_index()  # endpoint -1 에 대한 특허는 바로 weight가 해당 kp\n",
    "    step_4 = step_4[['source', 'weight']]  # kp Dataframe에 추가하기 위한 column 선택\n",
    "    step_4.columns = ['target', 'kp']  # 위의 line에서 컬럼명 변경\n",
    "    KP = pd.concat([KP, step_4])  # KP 업데이트\n",
    "    KP = KP.groupby(['target'])['kp'].sum().reset_index()\n",
    "\n",
    "    # end point에서\n",
    "    step_4 = step_3[~step_3['countif'].isnull()]\n",
    "    step_4 = step_4[['source', 'target', 'weight']]\n",
    "    step_5 = step_4.groupby(['source'])['target'].count().reset_index()\n",
    "    step_5.columns = ['target', 'countif']\n",
    "    step_4 = step_4.merge(step_5, on=\"target\", how=\"left\")\n",
    "    step_5 = step_4[step_4['countif'].isnull()]\n",
    "    step_5 = step_5.merge(KP, on='target', how='left')\n",
    "    step_5['kp'] = step_5['weight'] * step_5['kp']\n",
    "    step_5 = step_5[['source', 'kp']]\n",
    "    step_5.columns = ['target', 'kp']\n",
    "    KP = pd.concat([step_5, KP])\n",
    "    KP = KP.groupby(['target'])['kp'].sum().reset_index()\n",
    "    KP\n",
    "    i = 0\n",
    "    while True:\n",
    "        step_4 = step_4[~step_4['countif'].isnull()]\n",
    "        step_4 = step_4[['source', 'target', 'weight']]\n",
    "        step_5 = step_4.groupby(['source'])['target'].count().reset_index()\n",
    "        step_5.columns = ['target', 'countif']\n",
    "        step_4 = step_4.merge(step_5, on=\"target\", how=\"left\")\n",
    "        step_5 = step_4[step_4['countif'].isnull()]\n",
    "        step_5 = step_5.merge(KP, on='target', how='left')\n",
    "        step_5['kp'] = step_5['weight'] * step_5['kp']\n",
    "        step_5 = step_5[['source', 'kp']]\n",
    "        step_5.columns = ['target', 'kp']\n",
    "        KP = pd.concat([step_5, KP])\n",
    "        KP = KP.groupby(['target'])['kp'].sum().reset_index()\n",
    "        i = i + 1\n",
    "        print(i, \" : \", len(step_4))\n",
    "        if len(step_4) == 0:\n",
    "            break\n",
    "    print(KP)\n",
    "    print(\"--------------------------------\")\n",
    "    print(step_4.sort_values(['countif'], ascending=False))\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"time :\", time.time() - start)\n",
    "    print(\"KP type : \", type(KP))\n",
    "\n",
    "    return KP\n",
    "\n",
    "def lst_to_kp(edges):\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Convert list into pd.DataFrame\n",
    "    source = []\n",
    "    target = []\n",
    "    for edge in edges:\n",
    "        source.append(edge[0])\n",
    "        target.append(edge[1])\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['source'] = source\n",
    "    df['target'] = target\n",
    "\n",
    "    return df_to_kp(df)\n",
    "\n",
    "\n",
    "def get_values_df(lst:list, df:pd.DataFrame,col:str)->pd.DataFrame:\n",
    "    # List를 입력받고\n",
    "    # df에서 해당 List값에 대응하는 Column의 값을 찾아서\n",
    "    # List값과 대응되는 값의 목록을 df화 하여 반환\n",
    "    # 이 때, list의 length와 반환되는 df의 length는 같아야 한다.\n",
    "    # 엑셀의 vlookup 과 비슷한 느낌    \n",
    "    if not isinstance(lst,list):\n",
    "        raise TypeError\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError\n",
    "    if not isinstance(col, str):\n",
    "        raise TypeError\n",
    "    if len(df.columns) != 2:\n",
    "        print(\"데이터프레임 칼럼 수를 2로 맞춰주세요\")\n",
    "        raise ValueError\n",
    "    if not (col in df.columns):\n",
    "        print(\"데이터 프레임에 포함되지 않은 칼럼 이름입니다.\")\n",
    "        raise ValueError\n",
    "\n",
    "  # 기존의 데이터를 딥카피해서 정리\n",
    "    node_lst = lst.copy()\n",
    "    node_df = pd.DataFrame()\n",
    "    date_df = df.copy()\n",
    "\n",
    "  # 기존의 노드 목록에 추가정보를 붙임\n",
    "    node_df[col] = node_lst\n",
    "    left_df = node_df.merge(date_df, on=col,how=\"left\")\n",
    "    inner_df= node_df.merge(date_df, on=col,how=\"inner\")\n",
    "    if len(left_df) != len(inner_df):\n",
    "        print(\"데이터프레임에 없는 값이 리스트로 들어갔습니다!\")\n",
    "        raise ValueError\n",
    "    return inner_df\n",
    "\n",
    "def prepare_dates(df:pd.DataFrame, date_col:str, return_col:str):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError\n",
    "    if not isinstance(date_col, str):\n",
    "        raise TypeError\n",
    "\n",
    "    date_df = df.copy()\n",
    "    date_df.sort_values(date_col, ascending=False, inplace=True)\n",
    "\n",
    "    codes_lst = []\n",
    "    for i in date_df.loc[:,return_col]:\n",
    "        codes_lst.append(i)\n",
    "        return codes_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653bb2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# print(len(df))\n",
    "# print(len(source_pat),\"+\", len(sink_pat),\"+\",len(inter_pat),\"+\",len(isolated_pat), \" = \",len(source_pat)+ len(sink_pat)+len(inter_pat)+len(isolated_pat))\n",
    "\n",
    "def to_dataframe(st:set):\n",
    "    if type(st) == list:\n",
    "        tmp_lst = st.copy()\n",
    "        if not isinstance(st, list):\n",
    "            raise TypeError\n",
    "    elif type(st) == set:\n",
    "        tmp_lst = list(st)\n",
    "        if not isinstance(st,set):\n",
    "            raise TypeError\n",
    "    else:\n",
    "        print( \"list 혹은 set으로 데이터를 입력해주세요\")\n",
    "        raise ValueError\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    tmp_lst\n",
    "\n",
    "    df['target'] = tmp_lst\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_rank(df:pd.DataFrame):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError\n",
    "    if len(df.columns) != 1:\n",
    "        print(\"데이터프레임의 칼럼 갯수는 하나여야 합니다.\")\n",
    "        raise ValueError\n",
    "\n",
    "    tmp_df = df.copy()\n",
    "    counter = 0\n",
    "    tmp_lst = []\n",
    "    for i in tmp_df.index:\n",
    "        counter += 1\n",
    "        tmp_lst.append(counter)\n",
    "    tmp_df.loc[:,\"rank\"] = tmp_lst\n",
    "\n",
    "    return tmp_df\n",
    "\n",
    "def get_related(edges:pd.DataFrame, nodes:set, search_cl=\"target\", target_cl=\"source\")->list:\n",
    "    # 두 value의 관계를 정의한 edges 정보가 있고\n",
    "    # 원하는 한쪽의 정보(nodes)가 있을 때\n",
    "    # df상에서 해당 nodes와 대응되는 값을 list로 반환하는 함수\n",
    "    if not isinstance(edges, pd.DataFrame):\n",
    "        raise TypeError(\"edges should be a DataFrame\")\n",
    "    if not (isinstance(nodes, set) or isinstance(nodes, list)):\n",
    "        raise TypeError(\"nodess should be a set or list\")\n",
    "    if not ((search_cl in edges.columns) or (target_cl in edges.columns)):\n",
    "        print(\"분석을 하고 싶은 열(column) 이름을 정확하게 넣어 주십시오\")\n",
    "        raise TypeError\n",
    "\n",
    "    edges_df = edges.copy()\n",
    "    nodes_df = to_dataframe(nodes)\n",
    "    nodes_df.columns = [search_cl]\n",
    "    edges_df = edges_df.merge(nodes_df, on=search_cl, how='inner')\n",
    "    tmp_lst = list(set(edges_df[target_cl]))\n",
    "\n",
    "    return tmp_lst\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cycle을 기반으로 분석\n",
    "def get_values_df(lst:list, df:pd.DataFrame,col:str)->pd.DataFrame:\n",
    "    if not isinstance(lst,list):\n",
    "        raise TypeError\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError\n",
    "    if not isinstance(col, str):\n",
    "        raise TypeError\n",
    "    if len(df.columns) != 2:\n",
    "        print(\"데이터프레임 칼럼 수를 2로 맞춰주세요\")\n",
    "        raise ValueError\n",
    "    if not (col in df.columns):\n",
    "        print(\"데이터 프레임에 포함되지 않은 칼럼 이름입니다.\")\n",
    "        raise ValueError\n",
    "\n",
    "  # 기존의 데이터를 딥카피해서 정리\n",
    "    node_lst = lst.copy()\n",
    "    node_df = pd.DataFrame()\n",
    "    date_df = df.copy()\n",
    "\n",
    "  # 기존의 노드 목록에 추가정보를 붙임\n",
    "    node_df[col] = node_lst\n",
    "    left_df = node_df.merge(date_df, on=col,how=\"left\")\n",
    "    inner_df= node_df.merge(date_df, on=col,how=\"inner\")\n",
    "    if len(left_df) != len(inner_df):\n",
    "        print(\"데이터프레임에 없는 값이 리스트로 들어갔습니다!\")\n",
    "        raise ValueError\n",
    "    return inner_df\n",
    "\n",
    "def prepare_dates(df:pd.DataFrame, date_col:str, return_col:str):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError\n",
    "    if not isinstance(date_col, str):\n",
    "        raise TypeError\n",
    "\n",
    "    date_df = df.copy()\n",
    "    date_df.sort_values(date_col, ascending=False, inplace=True)\n",
    "\n",
    "    codes_lst = []\n",
    "    for i in date_df.loc[:,return_col]:\n",
    "        codes_lst.append(i)\n",
    "    return codes_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8686a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainpath_findout(EV_df, citation_df, gp_cutoff = 0.3, lp_cutoff = 0.7, condition_1_name = \"test\", condition_2_name = \"test\", condition_3_name = \"tmp\"):\n",
    "    G = nx.from_pandas_edgelist(citation_df, source='source', target='target', create_using=nx.DiGraph())\n",
    "\n",
    "    # 순환 구조 확인\n",
    "    cycles = list(nx.simple_cycles(G))\n",
    "    while cycles:\n",
    "        if cycles:  # If there are cycles\n",
    "            # Remove the first edge of the first cycle\n",
    "            G.remove_edge(cycles[0][0], cycles[0][1])\n",
    "\n",
    "            # Check if the graph still contains cycles\n",
    "            cycles = list(nx.simple_cycles(G))\n",
    "            if cycles:\n",
    "                print(\"The graph still contains cycles:\")\n",
    "                for cycle in cycles:\n",
    "                    print(cycle)\n",
    "                print()\n",
    "            else:\n",
    "                print(\"The graph doesn't contain any cycle after removing an edge.\")\n",
    "\n",
    "    # Extract edges from the graph\n",
    "    edges = G.edges()\n",
    "\n",
    "    # Convert the edges to a DataFrame\n",
    "    citation_df = pd.DataFrame(list(edges), columns=['source', 'target'])\n",
    "\n",
    "    kp_df = df_to_kp(citation_df)\n",
    "\n",
    "    EV_df = EV_df.merge(kp_df, left_on=\"등록번호\", right_on = \"target\", how= 'left')\n",
    "\n",
    "    from_to_lst = [i for i in G.edges]\n",
    "\n",
    "\n",
    "    g2 = nx.DiGraph()\n",
    "    g2.add_edges_from(from_to_lst)\n",
    "\n",
    "    # dag여부 판별\n",
    "\n",
    "    g2 = nx.DiGraph()\n",
    "    g2.add_edges_from(from_to_lst)\n",
    "\n",
    "    print(\"Directed 여부 : \",g2.is_directed())\n",
    "    print(\"Acyclic 여부 : \", nx.is_directed_acyclic_graph(g2))\n",
    "\n",
    "\n",
    "    EV_df['kp'] = EV_df['kp'].replace(np.nan, 0)\n",
    "    kp_df = EV_df.loc[:, ['등록번호', 'kp']].copy()\n",
    "    kp_df.columns = ['target', 'kp']\n",
    "\n",
    "    node_df = kp_df\n",
    "\n",
    "\n",
    "    # 레이어를 구성해서 모든 노드들을 레이어에 배치\n",
    "    # 가장 길이가 긴 경로를 구하고, 이를 기반으로 레이어 설정 \n",
    "    g2 = nx.DiGraph()\n",
    "    g2.add_edges_from(from_to_lst)\n",
    "\n",
    "    longest_path = nx.dag_longest_path(g2)\n",
    "    print(\"-\"*10,\"Longest Path\" ,\"-\"*10)\n",
    "    display(longest_path)\n",
    "    print(\"length of the longest path : \", len(longest_path))\n",
    "\n",
    "    tmp_df = to_dataframe(longest_path)\n",
    "    layer_df = get_rank(tmp_df)\n",
    "    layer_df.columns = [\"target\", \"layer\"]\n",
    "\n",
    "\n",
    "\n",
    "    edges_df = citation_df\n",
    "    df = EV_df\n",
    "\n",
    "    #source, sink여부 확인해서 node_df에 붙이기\n",
    "    source_pat = set(edges_df['source']) - set(edges_df['target'])\n",
    "    sink_pat = set(edges_df['target']) - set(edges_df['source'])\n",
    "    connected_pat = set(edges_df['target']) | set(edges_df['source'])\n",
    "    inter_pat = connected_pat - source_pat - sink_pat\n",
    "    isolated_pat = set(df['등록번호']) - connected_pat\n",
    "\n",
    "    node_analsys = [source_pat, sink_pat, inter_pat,isolated_pat]\n",
    "\n",
    "\n",
    "    tmp_lst =[]\n",
    "    for i in node_df.loc[:,\"target\"]:\n",
    "        if i in source_pat:\n",
    "            tmp_lst.append(\"source\")\n",
    "        elif i in sink_pat:\n",
    "            tmp_lst.append(\"sink\")\n",
    "        elif i in inter_pat:\n",
    "            tmp_lst.append(\"intermediate\")\n",
    "        else:\n",
    "            tmp_lst.append(\"isolated\")\n",
    "    node_df.loc[:, 'node_type'] = tmp_lst\n",
    "\n",
    "    print(\"layer가 부여된 노드의 갯수 : \", len(layer_df))\n",
    "    print(\"connected된 노드의 갯수 : \", len(connected_pat))\n",
    "\n",
    "    # source에 layer1을 부여하기\n",
    "    layer_df = to_dataframe(source_pat)\n",
    "    layer_n = 1\n",
    "    layer_df.loc[:,\"layer\"] = layer_n\n",
    "    layer_df.columns = ['target', 'layer']\n",
    "\n",
    "\n",
    "    # layer1과 연결된 노드들을 검증하기\n",
    "    layer_n += 1\n",
    "    former_layer = source_pat.copy()\n",
    "    edges = edges_df.copy()\n",
    "\n",
    "    while True:\n",
    "      # 이전 레이어와 연결된 노드들을 찾은 다음, 그 노드가 직전 노드만으로 전부 설명이 되면 레이어로 확정하기\n",
    "        print(\"\")\n",
    "        print(\"searching layer number: \", layer_n, \" of \", len(longest_path))\n",
    "        connected_lst = get_related(edges, former_layer, \"source\", \"target\") #n번째 레이어와 연결된 노드들 탐색\n",
    "        print(\"searched nodes that is connected to \",len(connected_lst))\n",
    "\n",
    "\n",
    "        not_nexts = []\n",
    "        counter = 0\n",
    "        for connected_node in connected_lst: # 찾고싶은 노드들\n",
    "            counter += 1\n",
    "    #         print(len(connected_lst),\" : \", counter)\n",
    "            bwd_lst = get_related(edges, [connected_node], \"target\", \"source\") # 반대로 source만 찾아서 붙이기\n",
    "            for bwd_node in bwd_lst:\n",
    "                if not (bwd_node in former_layer):\n",
    "                    not_nexts.append(connected_node) # 설명 안되는 노드가 존재?\n",
    "                    break\n",
    "        nexts = [i for i in connected_lst if not (i in not_nexts)]\n",
    "\n",
    "        print(\"former_layer: \", len(layer_df))\n",
    "        print(\"new layer: \", len(nexts))\n",
    "      #확실해진 다음 레이어들을 추가\n",
    "        tmp_df = to_dataframe(nexts)\n",
    "        tmp_df.loc[:,\"layer\"] = layer_n\n",
    "        tmp_df.columns = ['target', 'layer']\n",
    "        layer_df = pd.concat([layer_df, tmp_df], axis=0)\n",
    "        print(\"renewed layer: \", len(layer_df), \" of \", len(node_df) - len(isolated_pat))\n",
    "\n",
    "        if len(layer_df) == len(connected_pat):\n",
    "            print(\"condition fulfilled\")\n",
    "            break\n",
    "\n",
    "        if len(layer_df) > len(node_df):\n",
    "            print(\"레이어의 누적 갯수가 기존 노드를 뛰어넘었습니다. 재검토 필요\")\n",
    "            raise ValueError\n",
    "\n",
    "      # 이전 레이어에 있던 항목 연결들 제거\n",
    "        print(\"former_connections: \", len(edges))\n",
    "        tmp = []\n",
    "        for edge_idx in edges.index:\n",
    "            if edges.loc[edge_idx, 'source'] in former_layer:\n",
    "                tmp.append(edge_idx)\n",
    "        edges.drop(tmp, axis=0, inplace=True)  \n",
    "        print(\"renewed_connections: \", len(edges))\n",
    "\n",
    "        former_layer = nexts.copy()\n",
    "        layer_n += 1\n",
    "\n",
    "    # 레이어 정보를 노드에 추가\n",
    "    node_df = node_df.merge(layer_df,on='target',how='left')\n",
    "    node_df = node_df.fillna(0)\n",
    "\n",
    "\n",
    "    # 레이어로 GP를 찾음\n",
    "    layers = set(node_df['layer'])\n",
    "\n",
    "    tot_df = pd.DataFrame()\n",
    "    for layer in layers:\n",
    "        tmp_df = node_df[node_df['layer'] == layer].copy()\n",
    "        top_kp = tmp_df.sort_values('kp', ascending=False).iloc[0].kp\n",
    "        print(top_kp)\n",
    "        if top_kp == 0:\n",
    "            tmp_df['lp'] = 0\n",
    "        else:\n",
    "            tmp_df['lp'] = tmp_df['kp'] / top_kp\n",
    "        tot_df = pd.concat([tot_df, tmp_df], ignore_index=True)\n",
    "    node_df = tot_df.copy()\n",
    "\n",
    "    top_kp = max(node_df['kp'])\n",
    "    node_df['gp'] = node_df['kp'] / top_kp\n",
    "    print(\"top_kp : \", top_kp)\n",
    "\n",
    "    #cutoff로 HPP(High Persistence Patent) 판별하기\n",
    "    gp_tmp = node_df['gp'] > gp_cutoff\n",
    "\n",
    "    lp_tmp = node_df['lp'] > lp_cutoff\n",
    "\n",
    "\n",
    "    node_df.loc[:,\"is_HPP\"] = (gp_tmp | lp_tmp)\n",
    "\n",
    "\n",
    "    hpp_df = node_df[node_df['is_HPP'] == True].copy()\n",
    "    edges = edges_df.copy()\n",
    "\n",
    "    hpps = set(hpp_df.target)\n",
    "    mainpath = set()\n",
    "    tmps = set()\n",
    "\n",
    "    counter = 0\n",
    "    paths = []\n",
    "    for hpp in hpps:\n",
    "        path = []\n",
    "        path.append(hpp)\n",
    "        counter += 1\n",
    "        print(hpp, \" : \", len(hpps), \"중에 \", counter, \" 번째\")\n",
    "        while True:\n",
    "            print(\"path 길이 : \",len(path))\n",
    "            head = path[0]\n",
    "            hpp_related = get_related(edges, [head], \"target\", \"source\")\n",
    "            if len(hpp_related) != 0:\n",
    "                backward_top = get_values_df(hpp_related, node_df[['target', 'gp']], 'target').sort_values('gp', ascending=False).iloc[0,0]\n",
    "                path.insert(0, backward_top)\n",
    "            else:\n",
    "          # layer가 source쪽 끝에 도착\n",
    "                backward_top = \"\"\n",
    "                break\n",
    "\n",
    "        while True:\n",
    "            print(\"path 길이 : \",len(path))\n",
    "            tail = path[-1]\n",
    "            hpp_related = get_related(edges, [tail], \"source\", \"target\")\n",
    "            if len(hpp_related) != 0:      \n",
    "                forward_top = get_values_df(hpp_related, node_df[['target', 'gp']], 'target').sort_values('gp', ascending=False).iloc[0,0]\n",
    "                path.append(forward_top)\n",
    "            else:\n",
    "          # layer가 sink쪽 끝에 도착\n",
    "                forward_top = \"\"\n",
    "                break\n",
    "        paths.append(path)\n",
    "\n",
    "    elems = set()\n",
    "    for path in paths:\n",
    "        for elem in path:\n",
    "            elems.add(elem)\n",
    "\n",
    "    print(\"main path에 포함된 특허들 \",len(elems))\n",
    "\n",
    "    # node에다가 mainpath정보 추가\n",
    "    tmp_df = to_dataframe(list(elems))\n",
    "    tmp_df['is_mainpath'] = True\n",
    "    tmp_df.columns = ['target', 'is_mainpath']\n",
    "    node_df = node_df.merge(tmp_df, on='target',how=\"left\")\n",
    "    node_df['is_mainpath'] = node_df['is_mainpath'] == True\n",
    "\n",
    "    node_df.sort_values(['is_mainpath','is_HPP'], ascending = False)\n",
    "\n",
    "    EV_df = EV_df.merge(node_df.drop_duplicates(), left_on = '등록번호', right_on = 'target', how ='left')\n",
    "\n",
    "    tmp_condition_name1 = str(condition_1_name).replace(\"/\", \"\")\n",
    "    tmp_condition_name2 = str(condition_2_name).replace(\"/\", \"\")\n",
    "    tmp_condition_name3 = str(condition_3_name).replace(\"/\", \"\")\n",
    "    EV_df.to_excel(r'C:\\Users\\FILAB\\Desktop\\230711_자동차 산업\\230726_EV 분류별 MP\\\\' + str(tmp_condition_name1)+\"_\"+str(tmp_condition_name2)+\"_\"+str(tmp_condition_name3)+'_0.3gp.xlsx')\n",
    "    print(\"All Completed\")\n",
    "\n",
    "    return EV_df, node_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae945866",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_df['출원연도'] = EV_df['출원일'].fillna(\"\")\n",
    "EV_df['출원연도'] = EV_df['출원연도'].str.extract(r'(....)')\n",
    "EV_df = EV_df[EV_df['출원연도'].notna()]\n",
    "EV_df['출원연도'] = EV_df['출원연도'].astype('int')\n",
    "\n",
    "EV_df['출원연도']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9576324",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_df['CPC slash digits'] = EV_df['Current CPC Main'].str.extract(r\"(.*?(?=/))\")\n",
    "EV_df['CPC slash digits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf2ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_origin = citation_df.copy()\n",
    "EV_origin = EV_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10074bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa156ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143c6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_Case = EV_origin[['등록번호', 'Current CPC All']]\n",
    "tmp_Case = explode(tmp_Case.assign(citation = tmp_Case['Current CPC All'].str.split(\"|\")),'citation')\n",
    "tmp_Case = tmp_Case[['등록번호', 'citation']]\n",
    "tmp_Case['citation'] = tmp_Case['citation'].str.replace(\" \", \"\")\n",
    "tmp_Case['citation'] = tmp_Case['citation'].str.extract(r\"(.*?(?=/))\")\n",
    "tmp_Case.drop_duplicates(inplace = True)\n",
    "tmp_Case.groupby('citation').count().sort_values(by = '등록번호', ascending = False) / len(EV_origin)\n",
    "tmp_Case = tmp_Case.dropna()\n",
    "tmp_Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad8926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e4e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# 출원연도 선택\n",
    "itteration_conditions_1 = list(set(EV_origin['출원연도']))\n",
    "itteration_conditions_1 = [i for i in itteration_conditions_1 if i >= 2000] # 2000년대 이후의 연도만 선택\n",
    "itteration_conditions_1.sort()\n",
    "\n",
    "itteration_conditions_2 = set(tmp_Case['citation'])\n",
    "# itteration_conditions_2 = list(set(EV_origin[class_standard]))\n",
    "# itteration_conditions_3 = ['US', 'CN']\n",
    "\n",
    "years_set = set()\n",
    "\n",
    "EV_list = []\n",
    "df_list = []\n",
    "\n",
    "for class_chosen in itteration_conditions_2:\n",
    "    years_set = set()\n",
    "    for year_chosen in itteration_conditions_1:\n",
    "    #         for nation in itteration_conditions_3:            \n",
    "        years_set.add(year_chosen)\n",
    "        print(\"===================New Itteration Starts=================================\")\n",
    "        print(years_set)\n",
    "        print(class_chosen)\n",
    "        print()\n",
    "        citation_df = citation_origin.copy()\n",
    "        EV_df = EV_origin.copy()\n",
    "\n",
    "        EV_df = EV_df[EV_df['Current CPC All'].str.find(class_chosen) != -1]\n",
    "        EV_df = EV_df[EV_df['출원연도'].isin(years_set)]\n",
    "        \n",
    "        \n",
    "        application_filtered = set(EV_df['등록번호'])\n",
    "        tmp_df = citation_df.isin(application_filtered)\n",
    "        citation_df = citation_df[(tmp_df['source'] == True) & (tmp_df['target'] == True)]\n",
    "        print(str(len(citation_origin)) + \" 연결에서 \" + str(len(citation_df))+ \"개의 연결로\")\n",
    "\n",
    "        if len(citation_df) != 0:\n",
    "            # 메인패스 작동         \n",
    "            new_EV_df, node_df = mainpath_findout(EV_df, citation_df, condition_1_name = class_chosen, condition_2_name = year_chosen)\n",
    "\n",
    "            node_df = node_df[node_df['gp'] != 0]\n",
    "            node_df['year_chosen'] = year_chosen\n",
    "            node_df['class_chosen'] = class_chosen\n",
    "            df_list.append(node_df)\n",
    "            if year_chosen == 2023:\n",
    "                new_EV_df['class_chosen'] = class_chosen\n",
    "                EV_list.append(new_EV_df)\n",
    "\n",
    "print()\n",
    "print(\"iteration completed\")\n",
    "node_df = pd.concat(df_list, ignore_index = True)\n",
    "EV_df = pd.concat(EV_list, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8cf2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df.to_excel('230811_EV_from2000_yearstack.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_df.to_excel('230811_EV_from2000_2023.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_list\n",
    "# del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56954f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = pd.read_excel(\"230721_EV_yearstack.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42281a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = node_df.iloc[:, 1:]\n",
    "node_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04723c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf61d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_df_lst = []\n",
    "class_list = list(set(EV_df['소분류'].str.replace(\"\\/\",\"\")))\n",
    "print(class_list)\n",
    "\n",
    "for i in class_list:\n",
    "    print(i)\n",
    "    tmp_df = pd.read_excel(fr'230630_소분류 보고\\2022_{i}_0.3gp.xlsx',\n",
    "                          names = ['연번', '출원번호', '출원일', '출원연도', '출원인(정비)', '발명의명칭', '대표청구항', '요약', '대분류', '대분류코드', '중분류', '중분류코드', '소분류', '소분류코드', '국가코드', '특허청', '특허실용구분', '청구항수', '법적상태', '심사청구여부KRJPEP', '심사진행상태', '소멸이유', '분할출원구분', '공지예외주장여부', '정정공보여부', '심판횟수', '출원인', '출원인원문', '출원인국적', '출원인식별기호JP', '현재권리자', '현재권리자국적', '발명자', '발명자원문', '발명자국적', '제1출원인', '제1출원인원문', '제1출원인_정비', '제1출원인_분석', '제1출원인국적', '제1출원인국적_정비', '제1출원인국적_분석', '단독출원여부', '특허고객번호', '출원인수', '발명자수', '현재권리자수', '문헌종류코드', '분석구간', '공개번호', '공개일', '공고번호', '공고일', '등록번호', '등록일', '심사청구일KRJPEP', '존속기간만료일', '원출원번호', '원출원일자', '메인IPC', '전체IPC', '메인CPC', '전체CPC', '우선권국가', '우선권번호', '우선권주장일', '최우선일', '국제출원번호', '국제출원일', '국제공개번호', '국제공개일', '번역문제출일KR', '인용문헌번호', '인용문헌수', '국가별인용문헌수', '인용문헌국가', '자국인용문헌번호', '심사관인용문헌KRUS', '피인용문헌번호', '피인용문헌국가', '피인용문헌수', '피인용문헌수_분석', '국가별피인용문헌수', '비특허문헌수', '패밀리국가수', '패밀리문헌수', '패밀리문헌번호', '패밀리국가별문헌수', 'EPO심플패밀리문헌번호', 'EPO심플패밀리문헌번호_분석', 'EPO심플패밀리문헌수', 'EPO심플패밀리국가별문헌수', 'EPO심플패밀리국가수', '3극출원여부', '가설1', '코드', '임시', 'target_x', 'kp_x', 'target_y', 'kp_y', 'node_type', 'layer', 'lp', 'gp', 'is_HPP', 'is_mainpath'])\n",
    "#     print(len(tmp_df))\n",
    "    EV_df_lst.append(tmp_df)\n",
    "    print(len(EV_df.columns))\n",
    "\n",
    "tmp_df = pd.concat(EV_df_lst, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db242c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = node_df.merge(EV_df[['등록번호', '출원연도']].drop_duplicates(), left_on = 'target', right_on = \"등록번호\", how = \"left\")\n",
    "node_df.drop('등록번호', axis = 1, inplace = True)\n",
    "node_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f94697",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df.to_excel(\"230724_EV_TP_yearstack.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e952210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f7f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp값 뽑아내기\n",
    "\n",
    "# 이 기간 동안 'class'별 레코드 수를 계산합니다\n",
    "# EV_df기준 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653719c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6080908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pp_df_list = []\n",
    "next_n = 5\n",
    "\n",
    "tmp_df = EV_origin[EV_origin['문헌종류 코드'].isin(['B1', 'B2'])].copy()\n",
    "\n",
    "\n",
    "# class_counts = tmp_df[[class_standard, '출원연도']].value_counts()\n",
    "class_counts = tmp_df[['출원연도']].value_counts()\n",
    "\n",
    "class_counts = class_counts.reset_index()\n",
    "class_counts.columns = ['year_chosen', 'counts']\n",
    "class_counts = class_counts.sort_values(['year_chosen'])\n",
    "class_counts = class_counts.reset_index(drop=True)\n",
    "# 각 분류별로 그룹화하고, n년 동안의 합계를 계산합니다.\n",
    "result = class_counts['counts'].rolling(next_n).sum().reset_index(drop=True)\n",
    "class_counts['counts_next_n_years'] = result\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "display(class_counts)\n",
    "\n",
    "\n",
    "\n",
    "# 각 'code'의 최초 등장 연도를 찾습니다\n",
    "first_year = node_df[['target', '출원연도']].drop_duplicates()\n",
    "first_year.columns = ['target', 'year_chosen']\n",
    "first_kp = first_year.merge(node_df, on = ['target', 'year_chosen'], how = 'left')\n",
    "first_kp = first_kp[['target', 'year_chosen', 'kp']].fillna(0)\n",
    "\n",
    "first_kp['next_year'] = first_kp['year_chosen'] + 1\n",
    "\n",
    "# 1+5년 후의 데이터를 가져옵니다\n",
    "first_kp['year_plus_n'] = first_kp['next_year'] + next_n\n",
    "first_kp.columns = ['target', 'application_year', 'kp', 'next_year', 'year_plus_n']\n",
    "first_kp = first_kp[first_kp['application_year'] < (2022 - next_n)]\n",
    "\n",
    "\n",
    "df_n_yr = pd.merge(first_kp, node_df[['target', 'year_chosen', 'kp']], left_on=['target', 'next_year'], right_on=['target', 'year_chosen'], how='left')\n",
    "tmp = list(first_kp.columns)\n",
    "tmp.extend(['next_year_match', 'kp_next_year'])\n",
    "df_n_yr.columns = tmp\n",
    "df_n_yr = df_n_yr.fillna(0)\n",
    "\n",
    "df_n_yr = pd.merge(df_n_yr, node_df[['target', 'year_chosen', 'kp']], left_on=['target', 'year_plus_n'], right_on=['target', 'year_chosen'], how='inner')\n",
    "tmp.extend(['plus_n_match', 'kp_plus_n'])\n",
    "df_n_yr.columns = tmp\n",
    "df_n_yr = df_n_yr.fillna(0)\n",
    "\n",
    "# kp 변화량을 계산합니다\n",
    "df_n_yr['kp_change'] = df_n_yr['kp_plus_n'] - df_n_yr['kp_next_year']\n",
    "\n",
    "# 최종 결과를 출력합니다\n",
    "print(df_n_yr[['target', 'kp_change']])\n",
    "\n",
    "# 5년 동안의 count를 계산합니다\n",
    "tmp_class_counts = class_counts[['year_chosen', 'counts_next_n_years']]\n",
    "tmp_class_counts.columns = ['end_year', 'count']\n",
    "\n",
    "df_n_yr = pd.merge(df_n_yr, tmp_class_counts, left_on = 'year_plus_n', right_on = 'end_year', how='inner')\n",
    "df_n_yr['pp'] = (df_n_yr['kp_change'] / (next_n - 1)) / np.log(df_n_yr['count'])\n",
    "\n",
    "pp_df_list.append(df_n_yr)\n",
    "\n",
    "pp_df = pd.concat(pp_df_list, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d386ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d80c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_df.to_excel(\"230724_EV_pp_5yr.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7284fef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63781e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_df의 main path만 뽑고\n",
    "# 각 연도별 데이터 부여(없다면 해당 년도는 0으로 부여)\n",
    "# 각 년도별 gp로 순위 부여\n",
    "# 각 년도별 패러다임 조사\n",
    "# 그래프로 시각화, 각\n",
    "pp_df.drop_duplicates(inplace = True)\n",
    "report_df = pp_df[['target', 'pp']].merge(EV_df, left_on = ['target'], right_on = ['등록번호'], how = 'left')\n",
    "report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e31387",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.to_excel(\"230723_EV_total_pp.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential paradigm 점수 구하기\n",
    "# PP of Pat(i) = 최근 t년 기간의 시작과 끝 점 기준의 KP 기울기 / ln(최근 t년 동안의 특허 사이즈)\n",
    "# t=...3년?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6132439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corelation matix로 비교하기\n",
    "# Let's define a color map for the property\n",
    "color_map = {'장비·디바이스': 'blue', '플랫폼': 'green', '제조솔루션':'pink',\n",
    "             '생산현장': 'skyblue', '제어 및 측정장치': 'cyan', '제어시스템':'magenta',\n",
    "             '개발 및 운영시스템':'skyblue', '비즈니스 시스템':'cyan',\n",
    "             '데이터플랫폼':'cyan', '인프라':'magenta',\n",
    "             '3D 프린팅': 'red', '3D\\xa0프린팅':'red', 'AR/VR/MR': 'orange', '로봇': 'yellow', '머신비전': 'purple',\n",
    "             'CNC장비':'red', 'Motion Controller':'orange', '스마트 센서':'purple',\n",
    "             'DCS':'red','HMI':'orange','PLC':'yellow', 'SCADA':'purple',\n",
    "             'APS':'red', 'CAx':'orange', 'FEMS':'yellow', 'MES/MOM':'purple', 'PLM':'lime',\n",
    "             'ERP':'yellow', 'SCM':'purple',\n",
    "             'Big data/AI':'lime', 'Cloud computing':\"purple\", 'CPS/Digital twin':\"yellow\", 'Edge computing':\"orange\",\n",
    "             '5G 네트워크':'red', 'IIoT':\"lime\", '보안':\"yellow\"\n",
    "\n",
    "            }  # add more classes and colors if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb895f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc38062",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_df = pd.read_excel(r'230722_전체 MainPath\\2023_test_tmp_0.3gp.xlsx')\n",
    "EV_df = EV_df.iloc[:, 1:]\n",
    "EV_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# Main Path만 포함\n",
    "tmp_df = EV_df.copy()\n",
    "mp_df = tmp_df[tmp_df['is_mainpath'] == True].copy()\n",
    "\n",
    "citation_df = citation_origin.copy()\n",
    "\n",
    "# citation_df에서 main path관련 정보만 남김\n",
    "tmp_df = citation_origin.merge(mp_df, left_on ='source', right_on='등록번호', how='inner')\n",
    "tmp_df = tmp_df[['source', 'target']]\n",
    "tmp_df.columns = ['source', 'target']\n",
    "\n",
    "tmp_df = mp_df.merge(tmp_df, left_on='등록번호', right_on = 'target', how= 'inner')\n",
    "tmp_df = tmp_df[['source', 'target']]\n",
    "\n",
    "# # 익명화\n",
    "\n",
    "# nodes = set(tmp_df['source']).union(set(tmp_df['target']))\n",
    "# node_mapping = {node: f'N{i}' for i, node in enumerate(nodes)}\n",
    "\n",
    "# # 데이터프레임의 노드 번호를 새로운 코드로 업데이트하기\n",
    "# tmp_df['source'] = tmp_df['source'].apply(lambda x: node_mapping[x])\n",
    "# tmp_df['target'] = tmp_df['target'].apply(lambda x: node_mapping[x])\n",
    "\n",
    "# networkx 네트워크로 구성\n",
    "from_to_lst = [(row['source'], row['target']) for _, row in tmp_df.iterrows()]\n",
    "mp_nx = nx.DiGraph()\n",
    "mp_nx.add_edges_from(from_to_lst)\n",
    "\n",
    "\n",
    "# property 추가\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Suppose we have the following DataFrame\n",
    "\n",
    "# # Now, add properties to these nodes from the DataFrame\n",
    "# for idx, row in EV_df.iterrows():\n",
    "#     node = row['등록번호']\n",
    "#     if row[class_standard] == class_chosen:\n",
    "#         if node in mp_nx:\n",
    "#             mp_nx.nodes[node]['대분류'] = row['대분류']\n",
    "#             mp_nx.nodes[node]['중분류'] = row['중분류']\n",
    "#             mp_nx.nodes[node]['소분류'] = row['소분류']\n",
    "# # !@$!@%!@%%$@!%@! 여러 코드에 분류가 중복되는 경우????\n",
    "\n",
    "# # Check the properties of the nodes\n",
    "# for node, data in mp_nx.nodes(data=True):\n",
    "#     print(f\"Node: {node}, Data: {data}\")\n",
    "\n",
    "connected_subgraphs = list(nx.weakly_connected_components(mp_nx))\n",
    "# 연결된 부분 그래프의 개수 출력\n",
    "print(f\"연결된 부분 그래프의 개수: {len(connected_subgraphs)}\")\n",
    "\n",
    "# 각 연결된 부분 그래프 출력\n",
    "for i, subgraph in enumerate(connected_subgraphs):\n",
    "    print(f\"연결된 부분 그래프 {i+1}: {subgraph}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# print(classification_chosen)\n",
    "\n",
    "\n",
    "H = mp_nx\n",
    "\n",
    "\n",
    "# # Get a list of node properties\n",
    "# chosen_feature = class_standard\n",
    "# properties = [data[chosen_feature] if chosen_feature in data else 'missing' for node, data in H.nodes(data=True)]\n",
    "# colors = [color_map[property] if property in color_map else 'gray' for property in properties]\n",
    "\n",
    "# Figure size adjustment\n",
    "plt.figure(figsize=(20, 100))\n",
    "\n",
    "# Find the source nodes (nodes without incoming edges)\n",
    "source_nodes = [n for n, d in H.in_degree() if d==0]\n",
    "\n",
    "# Calculate node levels for each source node and BFS tree\n",
    "levels = defaultdict(list)\n",
    "for source_node in source_nodes:\n",
    "    bfs_tree = nx.bfs_tree(H, source_node)\n",
    "    level = nx.single_source_shortest_path_length(bfs_tree, source_node)\n",
    "    for node, lv in level.items():\n",
    "        levels[lv].append(node)\n",
    "\n",
    "# Create user-defined node positions\n",
    "pos = {node: (level, i/(len(nodes)-1)) if len(nodes) > 1 else (level, 0.5) for level, nodes in levels.items() for i, node in enumerate(nodes)}\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw(H, pos, with_labels=False, node_size=2000, font_size=15, arrowsize=20, edge_color=\"lightgray\")\n",
    "\n",
    "# Adjust node label positions\n",
    "label_pos = {k: [v[0], v[1]] for k, v in pos.items()}  # adjust the labels slightly above the nodes\n",
    "\n",
    "# Draw node labels\n",
    "nx.draw_networkx_labels(H, label_pos, font_size=15)\n",
    "\n",
    "try:\n",
    "    plt.savefig(f\"subgraph_{chosen_feature}_{class_chosen}.png\")\n",
    "except:\n",
    "    plt.savefig(f\"subgraph_tmp_.png\")\n",
    "\n",
    "# Display the graph\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2066639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# property 추가\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Suppose we have the following DataFrame\n",
    "\n",
    "# Now, add properties to these nodes from the DataFrame\n",
    "for idx, row in EV_df.iterrows():\n",
    "    node = row['등록번호']\n",
    "    if node in mp_nx:\n",
    "        mp_nx.nodes[node]['대분류'] = row['대분류']\n",
    "        mp_nx.nodes[node]['중분류'] = row['중분류']\n",
    "        mp_nx.nodes[node]['소분류'] = row['소분류']\n",
    "# !@$!@%!@%%$@!%@! 여러 코드에 분류가 중복되는 경우????\n",
    "        \n",
    "# Check the properties of the nodes\n",
    "for node, data in mp_nx.nodes(data=True):\n",
    "    print(f\"Node: {node}, Data: {data}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b52bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_df = EV_df\n",
    "# edges = citation_df\n",
    "# tmp_df = edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c558fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification = list(set(EV_df[class_standard]))\n",
    "class_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1a7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3047f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, subgraph in enumerate(connected_subgraphs):\n",
    "    H = mp_nx.subgraph(subgraph)\n",
    "\n",
    "    # Get a list of node properties\n",
    "    chosen_feature = '대분류'\n",
    "    properties = [data[chosen_feature] if chosen_feature in data else 'missing' for node, data in H.nodes(data=True)]\n",
    "    colors = [color_map[property] if property in color_map else 'gray' for property in properties]\n",
    "\n",
    "    # Figure size adjustment\n",
    "    plt.figure(figsize=(20, 20))\n",
    "\n",
    "    # Find the source nodes (nodes without incoming edges)\n",
    "    source_nodes = [n for n, d in H.in_degree() if d==0]\n",
    "\n",
    "    # Calculate node levels for each source node and BFS tree\n",
    "    levels = defaultdict(list)\n",
    "    for source_node in source_nodes:\n",
    "        bfs_tree = nx.bfs_tree(H, source_node)\n",
    "        level = nx.single_source_shortest_path_length(bfs_tree, source_node)\n",
    "        for node, lv in level.items():\n",
    "            levels[lv].append(node)\n",
    "\n",
    "    # Create user-defined node positions\n",
    "    pos = {node: (level, i/(len(nodes)-1)) if len(nodes) > 1 else (level, 0.5) for level, nodes in levels.items() for i, node in enumerate(nodes)}\n",
    "\n",
    "    # Draw the graph\n",
    "    nx.draw(H, pos, with_labels=False, node_size=2000, node_color=colors, font_size=15, arrowsize=20, edge_color=\"lightgray\")\n",
    "\n",
    "    # Adjust node label positions\n",
    "    label_pos = {k: [v[0], v[1]] for k, v in pos.items()}  # adjust the labels slightly above the nodes\n",
    "\n",
    "    # Draw node labels\n",
    "    nx.draw_networkx_labels(H, label_pos, font_size=15)\n",
    "\n",
    "    # Save the graph as a .png file\n",
    "    if 'classification_chosen' not in locals():\n",
    "        classification_chosen = \"tmp\"\n",
    "    plt.savefig(f\"subgraph_{chosen_feature}_{i}.png\")\n",
    "\n",
    "    # Display the graph\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851ec3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4548e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a826173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode(df, lst_cols, fill_value='', preserve_index=False):\n",
    "    # make sure `lst_cols` is list-alike\n",
    "    if (lst_cols is not None\n",
    "        and len(lst_cols) > 0\n",
    "        and not isinstance(lst_cols, (list, tuple, np.ndarray, pd.Series))):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "    # preserve original index values    \n",
    "    idx = np.repeat(df.index.values, lens)\n",
    "    # create \"exploded\" DF\n",
    "    res = (pd.DataFrame({\n",
    "                col:np.repeat(df[col].values, lens)\n",
    "                for col in idx_cols},\n",
    "                index=idx)\n",
    "             .assign(**{col:np.concatenate(df.loc[lens>0, col].values)\n",
    "                            for col in lst_cols}))\n",
    "    # append those rows that have empty lists\n",
    "    if (lens == 0).any():\n",
    "        # at least one list in cells is empty\n",
    "        res = (res.append(df.loc[lens==0, idx_cols], sort=False)\n",
    "                  .fillna(fill_value))\n",
    "    # revert the original index order\n",
    "    res = res.sort_index()\n",
    "    # reset index if requested\n",
    "    if not preserve_index:        \n",
    "        res = res.reset_index(drop=True)\n",
    "    return res\n",
    "\n",
    "# explode(df.assign(var1=df.var1.str.split(',')), 'var1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822fb59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for class_chosen in classes_list:\n",
    "    print(class_chosen.replace(\"/\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83527fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "assignee_list = []\n",
    "\n",
    "for class_chosen in classes_list:\n",
    "    print(class_chosen)\n",
    "    class_chosen_df = EV_df[EV_df[class_standard] == class_chosen]\n",
    "    #     print(len(class_chosen_df))\n",
    "\n",
    "    df = class_chosen_df\n",
    "\n",
    "    assignee_df = df.loc[:,['등록번호','출원인(정비)']]\n",
    "    assignee_df.columns = ['PatNum', 'assignee']\n",
    "    assignee_df = assignee_df[assignee_df['assignee'] != \" \"]\n",
    "\n",
    "    assignee_df = explode(assignee_df.assign(assignee = assignee_df.assignee.str.split(\"|\")),'assignee')\n",
    "\n",
    "    top_n = 30\n",
    "    top_n_assignee_df = assignee_df.groupby('assignee').count().sort_values('PatNum', ascending = False).iloc[0:top_n,:]\n",
    "    \n",
    "    top_n_assignee_df.to_excel('mp_top_'+str(top_n)+\"_list_of_\"+class_chosen.replace(\"/\",\"\")+\".xlsx\")\n",
    "    \n",
    "    top_n_assignee_df = top_n_assignee_df.reset_index()\n",
    "    assignee_df = assignee_df[assignee_df['assignee'].isin(set(top_n_assignee_df['assignee']))]\n",
    "    assignee_df = assignee_df.merge(EV_df[['등록번호', '출원연도', class_standard, '국가코드']].drop_duplicates(), left_on = 'PatNum', right_on = '등록번호')\n",
    "\n",
    "#     display(assignee_df)\n",
    "    assignee_list.append(assignee_df)\n",
    "\n",
    "assignee_df = pd.concat(assignee_list,ignore_index = True)\n",
    "assignee_df.to_excel(f\"{class_standard}_assignee_기초통계.xlsx\")\n",
    "    \n",
    "    # class_chosen_df\n",
    "assignee_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_chosen_df = class_chosen_df[class_chosen_df['is_mainpath'] == True].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22fc380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02ff36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d33f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(EV_df[class_standard]))\n",
    "\n",
    "tmp_lst = []\n",
    "\n",
    "for class_chosen in classes:\n",
    "    tmp_df = EV_df[EV_df[class_standard] == class_chosen].copy()\n",
    "    tmp_df = tmp_df[['등록번호', '출원연도', '국가코드']]\n",
    "    tmp_df.drop_duplicates(inplace = True)\n",
    "    tmp_df = tmp_df.groupby('출원연도').count()\n",
    "    tmp_df['class'] = class_chosen\n",
    "    tmp_df.reset_index(inplace = True)\n",
    "    \n",
    "    tmp_lst.append(tmp_df)\n",
    "    \n",
    "count_df = pd.concat(tmp_lst, ignore_index = True)\n",
    "count_df.columns = ['출원연도', '출원건수', '출원건수2', 'class']\n",
    "count_df = count_df[['출원연도', '출원건수', 'class']]\n",
    "count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cbc689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "df = EV_df.copy()\n",
    "\n",
    "assignee_df = df.loc[:,['등록번호','출원인(정비)']]\n",
    "assignee_df.columns = ['PatNum', 'assignee']\n",
    "assignee_df = assignee_df[assignee_df['assignee'] != \" \"]\n",
    "\n",
    "assignee_df = explode(assignee_df.assign(assignee = assignee_df.assignee.str.split(\"|\")),'assignee')\n",
    "assignee_df\n",
    "\n",
    "assignee_df = assignee_df.merge(EV_df[['등록번호', '출원연도', class_standard]].drop_duplicates(), left_on = \"PatNum\", right_on = '등록번호', how = \"left\")\n",
    "# assignee_df\n",
    "tmp_df = assignee_df.groupby(['출원연도', class_standard]).count()\n",
    "tmp_df.reset_index(inplace= True)\n",
    "tmp_df = tmp_df.iloc[:, :3]\n",
    "tmp_df.columns = ['출원연도', 'class', '출원인수']\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f3c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = count_df.merge(tmp_df, on = ['출원연도', 'class'], how = 'left')\n",
    "count_df\n",
    "\n",
    "count_df.to_excel(f'연도별 통계_{class_standard}_.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# classification_chosen기준으로 저장\n",
    "# top_n_assignee_df.to_excel('top_'+str(top_n)+\"_list_of_\"+classification_chosen+\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_df[['등록번호', '출원연도']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a37bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8447a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "total_df = pd.DataFrame()\n",
    "\n",
    "for class_chosen in set(EV_df['중분류']):\n",
    "    print(class_chosen)\n",
    "    df = EV_df.copy()\n",
    "    df = df[df['중분류'] == class_chosen]\n",
    "    \n",
    "    \n",
    "    assignee_df = df.loc[:,['등록번호','출원인(정비)']]\n",
    "    assignee_df.columns = ['PatNum', 'assignee']\n",
    "    assignee_df = assignee_df[assignee_df['assignee'] != \" \"]\n",
    "\n",
    "    assignee_df = explode(assignee_df.assign(assignee = assignee_df.assignee.str.split(\"|\")),'assignee')\n",
    "\n",
    "    top_n = 15\n",
    "    top_n_assignee_df = assignee_df.groupby('assignee').count().sort_values('PatNum', ascending = False).iloc[0:top_n,:]\n",
    "    top_n_assignee_df['class'] = class_chosen\n",
    "    \n",
    "    total_df = pd.concat([total_df, top_n_assignee_df])\n",
    "total_df.reset_index(inplace=True)    \n",
    "    \n",
    "total_df.to_excel('top_'+str(top_n)+\"_list_of_\"+\"classes.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835cc5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_assignees = total_df.groupby('class').apply(lambda x: x.nlargest(5, 'PatNum')['assignee'].tolist()).reset_index()\n",
    "\n",
    "# 출원인 리스트를 문자열로 변환\n",
    "top_assignees['top_5'] = top_assignees[0].apply(lambda x: ', '.join(x))\n",
    "top_assignees.drop(0, axis=1, inplace=True)\n",
    "\n",
    "print(top_assignees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_assignees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignee를 기준으로 연도별 출원 동향 보기56\n",
    "Annual_df = df[['등록번호', '등록일', '출원인']]\n",
    "Annual_df = Annual_df[Annual_df['등록일'] >'1990-01-01']\n",
    "Annual_df.loc[:, '등록일'] = pd.to_datetime(Annual_df['등록일']).dt.to_period('Y')\n",
    "Annual_df.columns = ['등록번호', '등록년도', 'assignee']\n",
    "\n",
    "\n",
    "\n",
    "Annual_df = Annual_df[Annual_df['assignee'] != \" \"]\n",
    "\n",
    "# Annual_df.assignee.str.split(\"|\")\n",
    "# \n",
    "Annual_df = explode(Annual_df.assign(assignee = Annual_df.assignee.str.split(\"|\")),'assignee')\n",
    "\n",
    "\n",
    "\n",
    "Annual_df = Annual_df.groupby(['등록년도', 'assignee']).count()\n",
    "Annual_df.reset_index(inplace = True)\n",
    "\n",
    "Annual_df.columns = ['등록년도', 'assignee', 'count']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Annual_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37432ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Annual_pv = Annual_df.groupby(['assignee', '등록년도']).sum('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ebd7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Annual_pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f92677",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = [20,15]\n",
    "# plot = Annual_pv.plot(figsize = set_size, style = 'o-')\n",
    "\n",
    "\n",
    "# for p in plot.patches:\n",
    "#     plot.annotate(str(p.get_height()), (p.get_x() * 0.999, p.get_height() * 1.005))\n",
    "# fig = plot.get_figure()\n",
    "# fig.savefig(directory_path + \"\\\\EDA\\\\\" + chosen_class +\"_line_\" + search_class + \".png\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#df.index = pd.to_datetime(df.index)  # If not a DatetimeIndex\n",
    "pts = Annual_pv.resample('1Y').asfreq()\n",
    "# pts\n",
    "fig, ax = plt.subplots(figsize = set_size)\n",
    "\n",
    "Annual_pv.plot(kind='line', ax=ax)\n",
    "# pts.plot(marker='o', ax=ax, lw=0, legend=False)\n",
    "\n",
    "pad = 1\n",
    "for idx, val in pts.stack().iteritems():\n",
    "    ax.annotate(val, (idx[0], val+pad))\n",
    "# https://stackoverflow.com/questions/55972583/plotting-multiple-lines-want-a-mark-every-with-a-text-label-on-each-line\n",
    "ax.grid(axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0257afd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
